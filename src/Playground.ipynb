{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LowProFool\n",
    "## Adversarial examples generation on the german credit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:48:14.033784Z",
     "start_time": "2024-10-28T09:48:14.024817Z"
    }
   },
   "outputs": [],
   "source": [
    "# Misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:48:14.809973Z",
     "start_time": "2024-10-28T09:48:14.804208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:48:16.596549Z",
     "start_time": "2024-10-28T09:48:16.455756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T09:48:37.637472Z",
     "start_time": "2024-10-28T09:48:17.451976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# Keras \n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "from Adverse import lowProFool, deepfool\n",
    "from Metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retina display\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "pd.set_option('display.max_columns', 500)\n",
    "tqdm.pandas()\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ccolors = [\"#008ae9\", \"#ea004f\"]\n",
    "sns.set_palette(ccolors)\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "DATASET = 'credit-g'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset):\n",
    "    assert(dataset == 'credit-g')\n",
    "    \n",
    "    dataset = fetch_openml(dataset)\n",
    "    target = 'target'\n",
    "    df = pd.DataFrame(data= np.c_[dataset['data'], dataset[target]], columns= dataset['feature_names'] + [target])  \n",
    "\n",
    "    # Renaming target for training later\n",
    "    df[target] = df[target].apply(lambda x: 0.0 if x == 'bad' or x == 0.0 else 1.0)\n",
    "\n",
    "    # Subsetting features to keep only continuous, discrete and ordered categorical\n",
    "    feature_names = ['checking_status', 'duration', 'credit_amount',\n",
    "                 'savings_status','employment','installment_commitment',\n",
    "                 'residence_since','age','existing_credits','num_dependents',\n",
    "                 'own_telephone','foreign_worker']\n",
    "\n",
    "    df = df[feature_names + [target]]\n",
    "\n",
    "    # Casting to float for later purpose\n",
    "    df = df.astype(float)\n",
    "    return df, target, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, target, feature_names, bounds):\n",
    "    df_return = df.copy()\n",
    "    \n",
    "    # Makes sure target does not need scaling\n",
    "    targets = np.unique(df[target].values)\n",
    "    assert(len(targets == 2) and 0. in targets and 1. in targets)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X = df_return[feature_names]\n",
    "    scaler.fit(X)    \n",
    "    df_return[feature_names] = scaler.transform(X)\n",
    "    \n",
    "    lower_bounds = scaler.transform([bounds[0]])\n",
    "    upper_bounds = scaler.transform([bounds[1]])\n",
    "\n",
    "    return scaler, df_return, (lower_bounds[0], upper_bounds[0])\n",
    "\n",
    "def get_weights(df, target, show_heatmap=False):\n",
    "    def heatmap(cor):\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "        plt.show()\n",
    "\n",
    "    cor = df.corr()\n",
    "    cor_target = abs(cor[target])\n",
    "\n",
    "    weights = cor_target[:-1] #removing target WARNING ASSUMES TARGET IS LAST\n",
    "    weights = weights / np.linalg.norm(weights)\n",
    "\n",
    "    if show_heatmap:\n",
    "        heatmap(cor)\n",
    "            \n",
    "    return weights.values\n",
    "\n",
    "def balance_df(df):\n",
    "    len_df_0, len_df_1 = len(df[df[target] == 0.]), len(df[df[target] == 1.])\n",
    "    df_0 = df[df[target] == 0.].sample(min(len_df_0, len_df_1), random_state=SEED)\n",
    "    df_1 = df[df[target] == 1.].sample(min(len_df_0, len_df_1), random_state=SEED)\n",
    "    df = pd.concat((df_0, df_1))\n",
    "    return df\n",
    "\n",
    "def get_bounds():\n",
    "    low_bounds = df_orig.min().values\n",
    "    up_bounds = df_orig.max().values\n",
    "    \n",
    "    #removing target WARNING ASSUMES TARGET IS LAST\n",
    "    low_bounds = low_bounds[:-1]\n",
    "    up_bounds = up_bounds[:-1]\n",
    "    \n",
    "    return [low_bounds, up_bounds]\n",
    "\n",
    "def split_train_test_valid():\n",
    "    # Train test splits\n",
    "    df_train, df_test = train_test_split(df, test_size=300, shuffle=True, random_state=SEED)\n",
    "    df_test, df_valid = train_test_split(df_test, test_size=50, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    return df_train, df_test, df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(conf, load=False):\n",
    "    assert(conf['Dataset'] == 'credit-g')\n",
    "    \n",
    "    class GermanNet(nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            super(GermanNet, self).__init__()\n",
    "            self.linear1 = torch.nn.Linear(D_in, H)\n",
    "            self.linear2 = torch.nn.Linear(H, H)\n",
    "            self.linear3 = torch.nn.Linear(H, D_out)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h1 = self.relu(self.linear1(x))\n",
    "            h2 = self.relu(self.linear2(h1))\n",
    "            h3 = self.relu(self.linear2(h2))\n",
    "            h4 = self.relu(self.linear2(h3))\n",
    "            h5 = self.relu(self.linear2(h4))\n",
    "            h6 = self.relu(self.linear2(h5))\n",
    "            a3 = self.linear3(h6)\n",
    "            y = self.softmax(a3)\n",
    "            return y\n",
    "\n",
    "    def train(model, criterion, optimizer, X, y, N, n_classes):\n",
    "        model.train()\n",
    "\n",
    "        current_loss = 0\n",
    "        current_correct = 0\n",
    "\n",
    "\n",
    "        # Training in batches\n",
    "        for ind in range(0, X.size(0), N):\n",
    "            indices = range(ind, min(ind + N, X.size(0)) - 1) \n",
    "            inputs, labels = X[indices], y[indices]\n",
    "            inputs = Variable(inputs, requires_grad=True)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            _, indices = torch.max(output, 1) # argmax of output [[0.61, 0.12]] -> [0]\n",
    "            # [[0, 1, 1, 0, 1, 0, 0]] -> [[1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0]]\n",
    "            preds = torch.tensor(keras.utils.to_categorical(indices, num_classes=n_classes))\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            current_correct += (preds.int() == labels.int()).sum() /n_classes\n",
    "\n",
    "\n",
    "        current_loss = current_loss / X.size(0)\n",
    "        current_correct = current_correct.double() / X.size(0)    \n",
    "\n",
    "        return preds, current_loss, current_correct.item()\n",
    "    \n",
    "    df = conf['TrainData']\n",
    "    target = conf['Target']\n",
    "    feature_names = conf['FeatureNames']\n",
    "                        \n",
    "    n_classes = len(np.unique(df[target]))\n",
    "    X_train = torch.FloatTensor(df[feature_names].values)\n",
    "    y_train = keras.utils.to_categorical(df[target], n_classes)\n",
    "    y_train = torch.FloatTensor(y_train)\n",
    "\n",
    "    D_in = X_train.size(1)\n",
    "    D_out = y_train.size(1)\n",
    "\n",
    "    epochs = 400\n",
    "    batch_size = 100\n",
    "    H = 100\n",
    "    net = GermanNet(D_in, H, D_out)\n",
    "\n",
    "    lr = 1e-4    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        preds, epoch_loss, epoch_acc = train(net, criterion, optimizer, X_train, y_train, batch_size, n_classes)     \n",
    "        if (epoch % 50 == 0):\n",
    "            print(\"> epoch {:.0f}\\tLoss {:.5f}\\tAcc {:.5f}\".format(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv(config, method):\n",
    "    df_test = config['TestData']\n",
    "    extra_cols = ['orig_pred', 'adv_pred', 'iters']    \n",
    "    model = config['Model']\n",
    "    weights = config['Weights']\n",
    "    bounds = config['Bounds']\n",
    "    maxiters = config['MaxIters']\n",
    "    alpha = config['Alpha']\n",
    "    lambda_ = config['Lambda']\n",
    "    \n",
    "    results = np.zeros((len(df_test), len(feature_names) + len(extra_cols)))    \n",
    "            \n",
    "    i = -1\n",
    "    for _, row in tqdm_notebook(df_test.iterrows(), total=df_test.shape[0], desc=\"{}\".format(method)):\n",
    "        i += 1\n",
    "        x_tensor = torch.FloatTensor(row[config['FeatureNames']])   \n",
    "        \n",
    "        if method == 'LowProFool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = lowProFool(x_tensor, model, weights, bounds,\n",
    "                                                             maxiters, alpha, lambda_)\n",
    "        elif method == 'Deepfool':\n",
    "            orig_pred, adv_pred, x_adv, loop_i = deepfool(x_tensor, model, maxiters, alpha,\n",
    "                                                          bounds, weights=[])\n",
    "        else:\n",
    "            raise Exception(\"Invalid method\", method)\n",
    "        results[i] = np.concatenate((x_adv, [orig_pred, adv_pred, loop_i]), axis=0)\n",
    "        \n",
    "    return pd.DataFrame(results, index=df_test.index, columns = feature_names + extra_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial dataset\n",
    "df_orig, target, feature_names = get_df(DATASET)\n",
    "\n",
    "# Balance dataset classes\n",
    "df = balance_df(df_orig)\n",
    "\n",
    "# Compute the bounds for clipping\n",
    "bounds = get_bounds()\n",
    "\n",
    "# Normalize the data\n",
    "scaler, df, bounds = normalize(df, target, feature_names, bounds)\n",
    "\n",
    "# Compute the weihts modelizing the expert's knowledge\n",
    "weights = get_weights(df, target)\n",
    "\n",
    "# Split df into train/test/valid\n",
    "df_train, df_test, df_valid = split_train_test_valid()\n",
    "\n",
    "# Build experimenation config\n",
    "config = {'Dataset'     : 'credit-g',\n",
    "         'MaxIters'     : 20000,\n",
    "         'Alpha'        : 0.001,\n",
    "         'Lambda'       : 8.5,\n",
    "         'TrainData'    : df_train,\n",
    "         'TestData'     : df_test,\n",
    "         'ValidData'    : df_valid,\n",
    "         'Scaler'       : scaler,\n",
    "         'FeatureNames' : feature_names,\n",
    "         'Target'       : target,\n",
    "         'Weights'      : weights,\n",
    "         'Bounds'       : bounds}\n",
    "\n",
    "# Train neural network\n",
    "model = get_model(config)\n",
    "config['Model'] = model\n",
    "\n",
    "# Compute accuracy on test set\n",
    "y_true = df_test[target]\n",
    "x_test = torch.FloatTensor(df_test[feature_names].values)\n",
    "y_pred = model(x_test)\n",
    "y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n",
    "print(\"Accuracy score on test data\", accuracy_score(y_true, y_pred))\n",
    "    \n",
    "# Sub sample\n",
    "config['TestData'] = config['TestData'].sample(n=10, random_state = SEED)\n",
    "\n",
    "# Generate adversarial examples\n",
    "df_adv_lpf = gen_adv(config, 'LowProFool')\n",
    "df_adv_df = gen_adv(config, 'Deepfool')\n",
    "config['AdvData'] = {'LowProFool' : df_adv_lpf, 'Deepfool' : df_adv_df}\n",
    "\n",
    "# Compute metrics\n",
    "list_metrics = {'SuccessRate' : True,\n",
    "                'iter_means': False,\n",
    "                'iter_std': False,\n",
    "                'normdelta_median': False,\n",
    "                'normdelta_mean': True,\n",
    "                'n_std': True,\n",
    "                'weighted_median': False,\n",
    "                'weighted_mean': True,\n",
    "                'w_std': True,\n",
    "                'mean_dists_at_org': False,\n",
    "                'median_dists_at_org': False,\n",
    "                'mean_dists_at_tgt': False,\n",
    "                'mean_dists_at_org_weighted': True,\n",
    "                'mdow_std': True,\n",
    "                'median_dists_at_org_weighted': False,\n",
    "                'mean_dists_at_tgt_weighted': True,\n",
    "                'mdtw_std': True,\n",
    "                'prop_same_class_arg_org': False,\n",
    "                'prop_same_class_arg_adv': False}\n",
    "\n",
    "all_metrics = get_metrics(config, list_metrics)\n",
    "all_metrics = pd.DataFrame(all_metrics, columns=['Method'] + [k for k, v in list_metrics.items() if v])\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ratios = []\n",
    "\n",
    "m_lpf = all_metrics[all_metrics.Method == 'LowProFool']\n",
    "m_df = all_metrics[all_metrics.Method =='Deepfool']\n",
    "\n",
    "sr = m_lpf.SuccessRate.values / m_df.SuccessRate.values \n",
    "wm =  m_lpf.weighted_mean.values / m_df.weighted_mean.values \n",
    "\n",
    "plot_ratios.append([100*sr[0], 100*wm[0]])\n",
    "plot_ratios = pd.DataFrame(plot_ratios, columns=['Success Rate Ratio', 'Mean Perturbation Ratio'])\n",
    "plot_ratios['Dataset'] = 'German Credit'\n",
    "\n",
    "f = plt.figure()\n",
    "ax = plt.axes()\n",
    "plot_ratios.plot(x='Dataset', kind='bar', legend=True, ax=ax)\n",
    "\n",
    "for i, v in enumerate(plot_ratios['Success Rate Ratio'].values):\n",
    "    ax.text(i - 0.2, v - 12 , str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "for i, v in enumerate(plot_ratios['Mean Perturbation Ratio'].values):\n",
    "    ax.text(i + 0.062, v - 12, str(v.round(1)) + '%', fontsize=16, color='white', weight='bold')\n",
    "\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center')\n",
    "ax.axhline(100, ls=':', c='grey')\n",
    "ax.text(-0.49, 100 - 5, '100%')\n",
    "\n",
    "\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
